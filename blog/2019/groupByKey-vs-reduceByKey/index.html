<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Tejas Lodaya | groupByKey vs reduceByKey in spark</title>
  <meta name="description" content="Tejas Lodaya">

  

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/blog/2019/groupByKey-vs-reduceByKey/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Tejas</strong> Lodaya
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- CV link -->
        <a class="page-link" href="/assets/pdf/CV.pdf">CV</a>

        <!-- Blog -->
        <a class="page-link" href="/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="/projects/">open source</a>
          
        
          
        
          
        

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">groupByKey vs reduceByKey in spark</h1>
    <p class="post-meta">May 2, 2019</p>
  </header>

  <article class="post-content">
    <ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#latency">Latency</a></li>
  <li><a href="#rdd">RDDs</a></li>
  <li><a href="#partitioning">Partitioning</a></li>
  <li><a href="#shuffling">Shuffling</a></li>
  <li><a href="#groupByKey">groupByKey</a></li>
  <li><a href="#reduceByKey">reduceByKey</a></li>
  <li><a href="#partitionby">Even faster reduceByKey</a></li>
  <li><a href="#takeaway">Takeaway</a></li>
  <li><a href="#references">References</a></li>
</ul>

<h3 id="introduction"> Introduction </h3>
<hr />
<p>Spark is a fast and general purpose cluster computing system hosted by Apache foundation. It provides general purpose distributed computing framework and high level APIs for Scala, Python and R. This post assumes working understanding of Spark internals like worker nodes, driver node, executors, cluster manager, RDDs, and the likes.</p>

<h3 id="latency"> Latency</h3>

<p><img src="/assets/img/groupbykey/latency.png" alt="latency" width="90%" height="90%" /></p>

<p>From the above diagram,</p>

<blockquote>
  <p>cache » memory (RAM) » network » disk.</p>
</blockquote>

<p>Spark triumphed over hadoop since hadoop used to write output of intermediate operations to disk, and read/write from disk for each operation. As seen, the latency was huge in case of hadoop. Spark was a bump over hadoop where all intermediate outputs were written to memory and read from memory.</p>

<h3 id="rdd"> Resilient Distributed Datasets </h3>
<hr />
<p>The main data storage unit of spark are RDDs. These store memory references to partitions of data stored on different nodes across a cluster. It contains two types of operations: transformations and actions. Transformations are lazy evaluation and actions are eager execution.</p>

<h3 id="partitioning"> Partitioning </h3>
<hr />
<p>Data when stored in RDDs is split across nodes of the cluster. The decision of which key-value pair goes to which node is decided by the partitioning logic. Each partitioning algorithm has its advantages and disadvantages.</p>

<ol>
  <li>Hash partitioning - Pass the key to a hash function to determine the node number to which the key will be passed to. This method attemps to spread data evenly across partitions.
    <blockquote>
      <p><code class="language-plaintext highlighter-rouge">val n = k.hashCode() % numNodes</code></p>
    </blockquote>
  </li>
  <li>Range partitioning - Therotically, define boundaries given a partition range to achieve uniform distribution across nodes. Practically, sample a small subset of keys to generate boundaries. Tuples with keys in the same range appear on the same machine.</li>
</ol>

<p>Some transformations like <code class="language-plaintext highlighter-rouge">map</code>, <code class="language-plaintext highlighter-rouge">flatMap</code> don’t inherit parent partition function (as the keys might have changed). Some other transformations like <code class="language-plaintext highlighter-rouge">mapValues</code> inherit parent partition function because it operates on the value only.</p>

<h3 id="shuffling"> Shuffling </h3>
<hr />
<p>Co-location significantly increases the performance of data-intensive applications. Shuffling helps achieve co-location, by moving grouped keys from one partition to another. While shuffling, spark uses partitioning to determine which key-value pair should be sent to which machine. Shuffle potentially creates data-skew where one partition contains a lot of data and the other doesn’t, leading to increased wait-times. <a href="https://www.youtube.com/watch?t=3215&amp;v=HG2Yd-3r4-M">This</a> helps identifying the problem, <a href="https://datarus.wordpress.com/2015/05/04/fighting-the-skew-in-spark/">this</a> helps solving the problem partially by creating dummy keys</p>

<h3 id="groupByKey"> groupByKey </h3>
<hr />
<p>This method brings all the keys belonging to the same group on one of the executor nodes by partitioning the hash value and pulling the result into memory to group as iterators. Each record whose key has the same hash value must live in memory on a single machine (co-location). In the word-count example, groupByKey results in same words shuffled to single partition. There’s no parallelism here and is strictly a sequential operation.</p>

<p><img src="https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/group_by.png" alt="" /></p>

<p>Disadvantages:</p>

<ol>
  <li>If just one of the keys contains too many records to fit in memory on one executor, the entire operation will fail and result in out-of-memory error (OOM)</li>
  <li>Co-location is nice to have, but is expensive.</li>
</ol>

<h3 id="reduceByKey"> reduceByKey </h3>
<hr />
<p>In case of associative operations like <code class="language-plaintext highlighter-rouge">sum</code>, <code class="language-plaintext highlighter-rouge">max</code>, there’s a faster way to achieve the result. Using the parallelism provided by spark, reduceByKey performs these reductions locally first (on executors), and then once again on the driver saving a ton of network traffic. In spark world, its also called <strong>“map-side reduce”</strong></p>

<p>Instead of sending all the data over the network, this method reduces it as small as it can and then send reductions over the wire. It can be distributed since each partition can be executed independent of the other partitions.</p>

<p>Taking a look at the <a href="https://github.com/apache/spark/blob/8cb23a1f9a3ed08e57865bcb6cc1cc7902881073/python/pyspark/rdd.py#L1466">source code</a>, <em>reduceByKey</em> is a specialization of <em>combineByKey</em> where, <em>mergeValues</em> and <em>mergeCombiners</em> are the same function.</p>

<p><img src="https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/reduce_by.png" alt="" /></p>

<h3 id="partitionby"> Even faster reduceByKey </h3>
<hr />
<p>Using reduceByKey instead of groupByKey localizes data better due to different partitioning strategies and thus reduces latency to deliver performance gains. We can improve the performance of reduceByKey even further by avoiding shuffling <em>altogether</em>. To achieve this, data has to be manually repartitioned using <code class="language-plaintext highlighter-rouge">partitionBy</code>. Efficient partitions can be created when keys belonging to the same group colocate on the same partition.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">events</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">'events.txt'</span><span class="p">)</span>
<span class="n">partitioner</span> <span class="o">=</span> <span class="n">RangePartitioner</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">events</span><span class="p">)</span>
<span class="n">eventsP</span> <span class="o">=</span> <span class="n">events</span><span class="p">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="n">partitioner</span><span class="p">).</span><span class="n">persist</span><span class="p">()</span>
<span class="n">eventsP</span><span class="p">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="n">add</span><span class="p">)</span>
</code></pre></div></div>

<p>This gives 9 times speed-ups in practical tests.</p>

<h3 id="takeaway"> Takeaway </h3>
<hr />
<ol>
  <li>The way data is organized on the cluster and the actions define latency of spark applications. in practical scenarios, you should spend considerable time finding optimal way of partitioning the data across nodes.</li>
  <li>Co-location can improve performance, but is hard to guarantee.</li>
  <li>Shuffling is data-intensive operation. Use it cautiously.</li>
</ol>

<h3 id="references"> References </h3>
<hr />
<ol>
  <li><a href="https://learning.oreilly.com/library/view/High+Performance+Spark/9781491943199/ch06.html#group_by_key">https://learning.oreilly.com/library/view/High+Performance+Spark/9781491943199/ch06.html#group_by_key</a></li>
  <li><a href="https://learning.oreilly.com/library/view/learning-spark/9781449359034/ch04.html">https://learning.oreilly.com/library/view/learning-spark/9781449359034/ch04.html</a></li>
  <li><a href="https://databricks.gitbooks.io/databricks-spark-knowledge-base/best_practices/prefer_reducebykey_over_groupbykey.html">https://databricks.gitbooks.io/databricks-spark-knowledge-base/best_practices/prefer_reducebykey_over_groupbykey.html</a></li>
</ol>


  </article>

  

</div>

      </div>
    </div>

    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">


<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-106825454-1', 'auto');
ga('send', 'pageview');
</script>



  </body>

</html>
